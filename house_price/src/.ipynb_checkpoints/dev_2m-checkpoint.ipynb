{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4432e93-eb5f-4889-958e-1cb6f7b24f54",
   "metadata": {},
   "source": [
    "## This is Dev notebook for house prices project from Kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3fcd3a5-9226-4206-a284-b1aff7c8a658",
   "metadata": {},
   "source": [
    "#### 1. Load environment and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929d9560-d703-4253-b941-024dfa3bcb65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, time, warnings, random, shap\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
    "from sklearn.svm import SVC, SVR\n",
    "from sklearn.preprocessing import LabelBinarizer, LabelEncoder, OrdinalEncoder, OneHotEncoder, StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression, Lasso, Ridge\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV, train_test_split, KFold\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier, VotingClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, r2_score, mean_squared_error\n",
    "from sklearn.inspection import permutation_importance\n",
    "from scipy.special import inv_boxcox\n",
    "from category_encoders import MEstimateEncoder\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('mode.chained_assignment', None)\n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def draw_histograms(df, variables, n_rows, n_cols):\n",
    "    # stolen from https://stackoverflow.com/questions/29530355/plotting-multiple-histograms-in-grid\n",
    "    fig=plt.figure()\n",
    "    for i, var_name in enumerate(variables):\n",
    "        ax=fig.add_subplot(n_rows,n_cols,i+1)\n",
    "        df[var_name].hist(bins=10,ax=ax)\n",
    "        ax.set_title(var_name+\" Distribution\")\n",
    "    fig.tight_layout()  \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def fillna_mp_i1(df_train, df_test, df_pred, num_features, cat_features, num_fill='median', cat_fill='mode'):\n",
    "    \"\"\"\n",
    "    This function speeds up filling missing values for 3 main datasets using different imputation methods.\n",
    "    Later may replace it with some subclass.\n",
    "    Example: fillna_mp_i1(X_train, X_test, X_pred, num_cols, cat_cols)\n",
    "    \"\"\"\n",
    "    # set df_pred to None if it does not exist\n",
    "    if (cat_fill=='mode'):\n",
    "    \n",
    "        df_train[cat_features] = df_train[cat_features].fillna(value=df_train[cat_features].mode().iloc[0])\n",
    "        df_test[cat_features] = df_test[cat_features].fillna(value=df_train[cat_features].mode().iloc[0])\n",
    "        if (df_pred is not None):\n",
    "            df_pred[cat_features] = df_pred[cat_features].fillna(value=df_train[cat_features].mode().iloc[0])\n",
    "            \n",
    "    if (cat_fill=='missing'):\n",
    "    \n",
    "        df_train[cat_features] = df_train[cat_features].fillna(value='missing')\n",
    "        df_test[cat_features] = df_test[cat_features].fillna(value='missing')\n",
    "        if (df_pred is not None):\n",
    "            df_pred[cat_features] = df_pred[cat_features].fillna(value='missing')\n",
    "        \n",
    "    if (num_fill=='median'):\n",
    "        df_train[num_features] = df_train[num_features].fillna(value=df_train[num_features].median())\n",
    "        df_test[num_features] = df_test[num_features].fillna(value=df_train[num_features].median())\n",
    "        if (df_pred is not None):\n",
    "            df_pred[num_features] = df_pred[num_features].fillna(value=df_train[num_features].median())    \n",
    "    \n",
    "    all_good = (\n",
    "    (np.prod(df_train[num_features+cat_features].shape)==df_train[num_features+cat_features].count().sum()) and \n",
    "    (np.prod(df_test[num_features+cat_features].shape) == df_test[num_features+cat_features].count().sum()) and \n",
    "    (np.prod(df_pred[num_features+cat_features].shape) == df_pred[num_features+cat_features].count().sum()))\n",
    "    if (all_good):\n",
    "        print('Missing values imputed successfully')\n",
    "    else:\n",
    "        print('There are still some missing values...')\n",
    "    \n",
    "    \n",
    "    \n",
    "def add_misDummy_mp_i1(df_train, df_test, df_pred, features):\n",
    "    \"\"\"\n",
    "    This function creates new dummy columns for missing features.\n",
    "    Example: add_misDummy_mp_i1(X_train, X_test, X_pred, ['Age'])\n",
    "    \"\"\"\n",
    "    # set df_pred to None if it does not exist\n",
    "    for feature_name in features:\n",
    "        misColName = 'mis'+feature_name\n",
    "        df_train.loc[df_train[feature_name].isnull(), misColName]=1\n",
    "        df_train.loc[df_train[feature_name].notnull(), misColName]=0\n",
    "        df_test.loc[df_test[feature_name].isnull(), misColName]=1\n",
    "        df_test.loc[df_test[feature_name].notnull(), misColName]=0\n",
    "        if (df_pred is not None):\n",
    "            df_pred.loc[df_pred[feature_name].isnull(), misColName]=1\n",
    "            df_pred.loc[df_pred[feature_name].notnull(), misColName]=0\n",
    "   \n",
    "\n",
    "def discretize_mp_i1(df_train, df_test, df_pred, feature, ntiles, delete_feature=False):\n",
    "    \"\"\"\n",
    "    This function divides a continuous feature into quantile groups.\n",
    "    Example: discretize_mp_i1(X_train, X_test, X_pred, 'Age', 15)\n",
    "    \"\"\"\n",
    "    # set df_pred to None if it does not exist\n",
    "    _,bin = pd.qcut(df_train[feature], ntiles, retbins = True, labels = False, duplicates = 'drop')\n",
    "    df_train[feature+'Ntile'] = pd.cut(df_train[feature], \n",
    "                                       labels=False, \n",
    "                                       duplicates = 'drop', \n",
    "                                       bins = bin , \n",
    "                                       include_lowest = True)\n",
    "    df_test[feature+'Ntile'] = pd.cut(df_test[feature], \n",
    "                                      labels=False, \n",
    "                                      duplicates = 'drop', \n",
    "                                      bins = bin , \n",
    "                                      include_lowest = True)\n",
    "    if (df_pred is not None):\n",
    "        df_pred[feature+'Ntile'] = pd.cut(df_pred[feature], \n",
    "                                          labels=False, \n",
    "                                          duplicates = 'drop', \n",
    "                                          bins = bin , \n",
    "                                          include_lowest = True)\n",
    "    if (delete_feature==True):\n",
    "        df_train.drop(columns=[feature], inplace=True)\n",
    "        df_test.drop(columns=[feature], inplace=True)\n",
    "        df_pred.drop(columns=[feature], inplace=True)\n",
    "    print('Discretized ',feature, ' into ', len(bin)-1, ' bins')\n",
    "\n",
    "\n",
    "def log_transformer_mp_i1(df_train, df_test, feature_subset=False, min_skew=3, df_pred=None):\n",
    "    \"\"\"\n",
    "    This function divides a continuous feature into quantile groups.\n",
    "    Example: log_transformer_mp_i1(X_train, X_test, X_pred, feature_subset=num_cols)\n",
    "    \"\"\"\n",
    "    # set df_pred to None if it does not exist\n",
    "    if (feature_subset==False):\n",
    "        features_totransform = df_train.columns\n",
    "    else:\n",
    "        features_totransform = feature_subset.copy()\n",
    "    skewed_vars = list(df_train.skew()[abs(df_train.skew())>min_skew].index)\n",
    "    for col in list(set(skewed_vars)&set(features_totransform)):\n",
    "        df_train[col] = np.log1p(df_train[col])\n",
    "        df_test[col] = np.log1p(df_test[col])\n",
    "        if df_pred:\n",
    "            df_pred[col] = np.log1p(df_pred[col])\n",
    "    print('Skewed columns log-transformed: ', list(set(skewed_vars)&set(features_totransform)))\n",
    "    \n",
    "    \n",
    "def add_dummyfeatures(df_train, df_test, feature_dict, df_pred=None):\n",
    "    \"\"\"\n",
    "    This function adds dummy feature when some feature is equal to value, specified in a dictionary.\n",
    "    Example: add_dummyfeatures(X_train, X_test, X_pred, {'RoomService':0, 'Spa':0, 'VRDeck':0, 'ShoppingMall':0})\n",
    "    \"\"\"\n",
    "    if df_pred:\n",
    "        input_dimensions = np.array([df_train.shape[1], df_test.shape[1], df_pred.shape[1]])\n",
    "    else:\n",
    "        input_dimensions = np.array([df_train.shape[1], df_test.shape[1]])    \n",
    "    for i in range(len(list(feature_dict.items()))):\n",
    "        feature,value = list(feature_dict.keys())[i], list(feature_dict.values())[i]\n",
    "        df_train.loc[df_train[feature]==value,(str(feature)+str(value))]=1\n",
    "        df_train.loc[df_train[feature]!=value,(str(feature)+str(value))]=0\n",
    "        df_test.loc[df_test[feature]==value,(str(feature)+str(value))]=1\n",
    "        df_test.loc[df_test[feature]!=value,(str(feature)+str(value))]=0\n",
    "        if df_pred:\n",
    "            df_pred.loc[df_pred[feature]==value,(str(feature)+str(value))]=1\n",
    "            df_pred.loc[df_pred[feature]!=value,(str(feature)+str(value))]=0\n",
    "    if df_pred:\n",
    "        output_dimensions = np.array([df_train.shape[1], df_test.shape[1], df_pred.shape[1]])\n",
    "    else:\n",
    "        output_dimensions = np.array([df_train.shape[1], df_test.shape[1]])\n",
    "    print(output_dimensions-input_dimensions, ' variables created') \n",
    "    \n",
    "\n",
    "### target encoding ###\n",
    "# source: https://www.kaggle.com/code/ryanholbrook/feature-engineering-for-house-prices/notebook\n",
    "\n",
    "class CrossFoldEncoder:\n",
    "    def __init__(self, encoder, **kwargs):\n",
    "        self.encoder_ = encoder\n",
    "        self.kwargs_ = kwargs  # keyword arguments for the encoder\n",
    "        self.cv_ = KFold(n_splits=4)\n",
    "\n",
    "    # Fit an encoder on one split and transform the feature on the\n",
    "    # other. Iterating over the splits in all folds gives a complete\n",
    "    # transformation. We also now have one trained encoder on each\n",
    "    # fold.\n",
    "    def fit_transform(self, X, y, cols):\n",
    "        self.fitted_encoders_ = []\n",
    "        self.cols_ = cols\n",
    "        X_encoded = []\n",
    "        for idx_encode, idx_train in self.cv_.split(X):\n",
    "            fitted_encoder = self.encoder_(cols=cols, **self.kwargs_)\n",
    "            fitted_encoder.fit(\n",
    "                X.iloc[idx_encode, :], y.iloc[idx_encode],\n",
    "            )\n",
    "            X_encoded.append(fitted_encoder.transform(X.iloc[idx_train, :])[cols])\n",
    "            self.fitted_encoders_.append(fitted_encoder)\n",
    "        X_encoded = pd.concat(X_encoded)\n",
    "        X_encoded.columns = [name + \"_encoded\" for name in X_encoded.columns]\n",
    "        return X_encoded\n",
    "\n",
    "    # To transform the test data, average the encodings learned from\n",
    "    # each fold.\n",
    "    def transform(self, X):\n",
    "        from functools import reduce\n",
    "\n",
    "        X_encoded_list = []\n",
    "        for fitted_encoder in self.fitted_encoders_:\n",
    "            X_encoded = fitted_encoder.transform(X)\n",
    "            X_encoded_list.append(X_encoded[self.cols_])\n",
    "        X_encoded = reduce(\n",
    "            lambda x, y: x.add(y, fill_value=0), X_encoded_list\n",
    "        ) / len(X_encoded_list)\n",
    "        X_encoded.columns = [name + \"_encoded\" for name in X_encoded.columns]\n",
    "        return X_encoded    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f170490-01fe-474c-a72e-665c994101cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "time0 = time.time()\n",
    "\n",
    "os.chdir('/home/jupyter/projects_data/house_price')\n",
    "df = pd.read_csv('train.csv') \n",
    "# df.drop(columns = ['Id'], inplace=True)\n",
    "pred=pd.read_csv('test.csv')\n",
    "pred0 = pred.copy()\n",
    "\n",
    "print(df.shape, pred.shape)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6328eb6-8677-4a3e-b6cf-39dea0cce6c3",
   "metadata": {},
   "source": [
    "#### 2. Data cleaning and EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42fb0e6-5457-4bde-9536-209f3f621490",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. pEDA #\n",
    "\n",
    "cols_tokeep = ['SalePrice', 'LotArea', 'OverallQual', 'OverallCond', 'YearBuilt', 'ExterCond', \n",
    "               'BsmtFinSF1', 'BsmtFinSF2', 'TotalBsmtSF', 'HeatingQC', '1stFlrSF', '2ndFlrSF', 'GrLivArea',  \n",
    "               'KitchenQual', 'GarageArea', 'GarageCars', 'TotRmsAbvGrd', 'BedroomAbvGr', 'FullBath', \n",
    "               'HalfBath', 'MiscVal', 'LotFrontage', \n",
    "               'ExterQual', 'MSSubClass', 'MSZoning', 'LotShape', 'LandContour', 'LotConfig', 'Neighborhood',\n",
    "               'Condition1', 'BldgType', 'HouseStyle', 'RoofStyle', 'Exterior1st', 'Exterior2nd',\n",
    "               'Foundation', 'Heating', 'CentralAir', 'Electrical', 'Functional', 'PavedDrive',\n",
    "               'SaleType', 'SaleCondition', 'BsmtQual', 'BsmtCond', \n",
    "               'BsmtExposure', 'BsmtFinType1', 'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond']\n",
    "df = df[cols_tokeep]\n",
    "\n",
    "# preliminary feature engineering:\n",
    "df['GrLivArea_log'] = np.log1p(df['GrLivArea'])\n",
    "pred['GrLivArea_log'] = np.log1p(pred['GrLivArea'])\n",
    "# w/o logtransform, scatterplot looks better. not sure whether log tranform helps.\n",
    "\n",
    "df['MisGarage'] = df.GarageType.isnull().astype(int)\n",
    "df['MisBsmt'] = df.BsmtCond.isnull().astype(int)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e04131-6d39-455e-820b-c6117ba16961",
   "metadata": {},
   "outputs": [],
   "source": [
    "ord_cols = ['ExterCond', 'HeatingQC', 'KitchenQual', 'ExterQual', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond']\n",
    "num_cols = ['LotArea', 'YearBuilt', 'BsmtFinSF1', 'BsmtFinSF2', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', \n",
    "            'GrLivArea', 'GarageArea', 'MiscVal', 'LotFrontage', \n",
    "           'TotRmsAbvGrd', 'GarageCars', 'BedroomAbvGr', 'OverallCond', 'OverallQual', 'GrLivArea_log']\n",
    "cat_cols = list(set(df.columns)-set(num_cols)-set(ord_cols)-set(['SalePrice']))\n",
    "print(\"Numerical features \", num_cols, \"\\n\",\n",
    "      'Ordinal features', ord_cols, '\\n',\n",
    "      \"Categorical features \", cat_cols)\n",
    "\n",
    "df[ord_cols] = df[ord_cols].replace(['Po', 'Fa', 'TA', 'Gd', 'Ex'], [1,2,3,4,5])\n",
    "pred[ord_cols] = pred[ord_cols].replace(['Po', 'Fa', 'TA', 'Gd', 'Ex'], [1,2,3,4,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c44c949-f9ed-475c-b52f-dc622682d4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[cat_cols].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f4cb9d-0f8a-4534-9567-2428dd650f58",
   "metadata": {},
   "source": [
    "According to feature importances, only Neighboorhood and possibly Exterior1 categorical features are really useful.\n",
    "And they have too many unique values to use OHC. All other categorical features are not important enough to bother with ohe.\n",
    "So use target encoding for all of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f495a1-13e1-4a81-8c53-d788b3ae359b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 0.2\n",
    "df.reset_index(inplace=True, drop=True)\n",
    "#random.seed(2)\n",
    "test_index = random.sample(list(df.index), int(test_size*df.shape[0]))\n",
    "train = df.iloc[list(set(df.index)-set(test_index))]\n",
    "test = df.iloc[test_index]\n",
    "train.reset_index(drop=True, inplace=True)\n",
    "test.reset_index(drop=True, inplace=True)\n",
    "train.drop(columns=['id'],inplace=True, errors='ignore')\n",
    "test.drop(columns=['id'],inplace=True, errors='ignore')\n",
    "display(train.shape, test.shape, train.head(3), test.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f671e1-f114-414c-8122-616e0a785d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill missing values\n",
    "display(train.info())\n",
    "\n",
    "mis_col_mode = ['LotFrontage', 'Electrical']\n",
    "mis_cat_cols = ['BsmtExposure', 'BsmtFinType1', 'GarageType', 'GarageFinish']\n",
    "mis_num_cols = ['BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond']\n",
    "\n",
    "for col in mis_col_mode:\n",
    "    train[col] = train[col].fillna(train[col].mode()[0])\n",
    "    test[col] = test[col].fillna(train[col].mode()[0])\n",
    "\n",
    "for col in mis_cat_cols:\n",
    "    train[col] = train[col].fillna(value='missing')\n",
    "    test[col] = test[col].fillna(value='missing')\n",
    "    \n",
    "for col in mis_num_cols:\n",
    "    train[col] = train[col].fillna(value=-1)\n",
    "    test[col] = test[col].fillna(value=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099bc4ff-f09a-4cc7-bfb6-cc52b4c7d539",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(x='SalePrice', data=train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4418bd4e-aaff-4d1f-9792-7c793d8fd6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x='SalePrice', y='Neighborhood', data = train)\n",
    "plt.show()\n",
    "sns.barplot(y='SalePrice', x='OverallQual', data = train)\n",
    "plt.show()\n",
    "sns.scatterplot(y='SalePrice', x='GrLivArea', data = train)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c265fa5-f54b-4a01-9f66-3c6ab55508c1",
   "metadata": {},
   "source": [
    "#### 3. Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75d062a-20ce-4cd5-8b8e-09d56bad64f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['SalePrice'] = np.log(train['SalePrice'])\n",
    "test['SalePrice'] = np.log(test['SalePrice'])\n",
    "\n",
    "# do target encoding #\n",
    "\n",
    "encoder = CrossFoldEncoder(MEstimateEncoder, m=10)\n",
    "train_encoded = encoder.fit_transform(train, train.SalePrice, cols=cat_cols)\n",
    "test_encoded = encoder.transform(test)\n",
    "\n",
    "train.drop(columns=cat_cols, inplace=True)\n",
    "test.drop(columns=cat_cols,  inplace=True)\n",
    "train = pd.concat([train, train_encoded], axis = 1)\n",
    "test = pd.concat([test, test_encoded], axis = 1)\n",
    "\n",
    "display(train.shape, train.head(), train.count())\n",
    "train0 = train.copy()\n",
    "test0 = test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95dadbc6-16a9-4299-99f1-5ba41c4a8f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_dummyfeatures(train, test, {'OverallQual':1})\n",
    "add_dummyfeatures(train, test, {'OverallQual':8})\n",
    "add_dummyfeatures(train, test, {'OverallQual':9})\n",
    "add_dummyfeatures(train, test, {'OverallQual':10})\n",
    "\n",
    "log_transformer_mp_i1(train, test, feature_subset=num_cols)\n",
    "\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b41d863-96c3-48e6-b04f-018a368175f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train.copy()\n",
    "y_train = X_train.pop('SalePrice')\n",
    "X_test = test.copy()\n",
    "y_test = X_test.pop('SalePrice')\n",
    "print(X_train.shape, X_test.shape, y_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8296fa14-ad15-432e-8db1-218789c957c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_transformer = ColumnTransformer([\n",
    "    (\"num\", StandardScaler(), num_cols+ord_cols),\n",
    "    ], remainder = \"passthrough\")\n",
    "\n",
    "X_train = pd.DataFrame(feature_transformer.fit_transform(X_train), \n",
    "                       columns=feature_transformer.get_feature_names_out())\n",
    "X_test = pd.DataFrame(feature_transformer.transform(X_test), \n",
    "                      columns=feature_transformer.get_feature_names_out())\n",
    "\n",
    "# there are many dummies... may wish to use pca here later.\n",
    "\n",
    "print(X_train.shape, X_test.shape, y_train.shape)\n",
    "display(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb43af20-f1d5-4ee6-9337-9a6b8e1afa14",
   "metadata": {},
   "source": [
    "#### 4. Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10028fa2-c9b6-446e-a998-a8bb9f1ff1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "print('OLS ', mean_squared_error(y_train, lr.predict(X_train))**0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d859343-301d-4059-8f4e-cd66bb0400a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbb = XGBRegressor(n_estimators=200,\n",
    "                   max_depth=5,\n",
    "                   eta=0.06,\n",
    "                   subsample=0.8,\n",
    "                   colsample_bytree=0.6)\n",
    "xgbb.fit(X_train, y_train)\n",
    "print('xgb ', mean_squared_error(y_train, xgbb.predict(X_train))**0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe2f1cf-515c-4fc0-aed4-7e616dae37ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbgs = XGBRegressor()\n",
    "grid_param = {'n_estimators':[200], \n",
    "              'max_depth':[2, 3, 4], \n",
    "              'eta':[0.05, 0.07, 0.09],\n",
    "              'subsample':[0.7], \n",
    "              'colsample_bytree':[0.5]}\n",
    "xgbgs = GridSearchCV(xgbgs, grid_param, cv=2, scoring='neg_root_mean_squared_error')\n",
    "xgbgs.fit(X_train, y_train)\n",
    "\n",
    "print('xgbgs ',       \n",
    "      xgbgs.best_params_, \n",
    "      xgbgs.best_score_, \n",
    "      mean_squared_error(y_train, xgbgs.predict(X_train))**0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38dbaa68-8199-4e34-8d3a-cfc067761edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.exp(y_train)\n",
    "y_test = np.exp(y_test)\n",
    "\n",
    "# performance evaluation: train set\n",
    "print('Train: ')\n",
    "print('OLS', np.sqrt(mean_squared_error(y_train, np.exp(lr.predict(X_train)))))\n",
    "print('XGB', np.sqrt(mean_squared_error(y_train, np.exp(xgbb.predict(X_train)))))\n",
    "print('XGBgs', np.sqrt(mean_squared_error(y_train, np.exp(xgbgs.predict(X_train)))))\n",
    "\n",
    "# performance evaluation: test set\n",
    "print('Test: ')\n",
    "print('OLS', np.sqrt(mean_squared_error(y_test, np.exp(lr.predict(X_test)))))\n",
    "print('XGB', np.sqrt(mean_squared_error(y_test, np.exp(xgbb.predict(X_test)))))\n",
    "print('XGBgs', np.sqrt(mean_squared_error(y_test, np.exp(xgbgs.predict(X_test)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6290ccc6-072e-47ed-b871-709f81848c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVR sucks, so I omit it.\n",
    "# as usual, XGB GS fails to clearly beat XGB baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb479be-973b-424e-88a9-c424640c7295",
   "metadata": {},
   "source": [
    "#### 5. Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a35f8b-8ab8-4349-a47f-e0288a108041",
   "metadata": {},
   "outputs": [],
   "source": [
    "# template here: https://www.kaggle.com/code/kaanboke/catboost-lightgbm-xgboost-explained-by-shap/notebook\n",
    "explainerxgbc = shap.TreeExplainer(xgbb)\n",
    "shap_values_XGBoost_test = explainerxgbc.shap_values(X_test)\n",
    "shap_values_XGBoost_train = explainerxgbc.shap_values(X_train)\n",
    "\n",
    "vals = np.abs(shap_values_XGBoost_test).mean(0)\n",
    "feature_names = X_test.columns\n",
    "feature_importance = pd.DataFrame(list(zip(feature_names, vals)),\n",
    "                                 columns=['col_name','feature_importance_vals'])\n",
    "feature_importance.sort_values(by=['feature_importance_vals'],\n",
    "                              ascending=False, inplace=True)\n",
    "#display(feature_importance)\n",
    "\n",
    "shap.summary_plot(shap_values_XGBoost_test, X_test, plot_type=\"bar\", plot_size=(6,6), max_display=20)\n",
    "shap.summary_plot(shap_values_XGBoost_train, X_train,plot_type=\"dot\", plot_size=(6,6), max_display=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a4ffce-167c-4d07-8714-1e719aa6d77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total time for this Dev script: ', time.time() - time0)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m98",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m98"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
