{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92625450-e98a-4f34-8b42-5bd9e361a644",
   "metadata": {
    "tags": []
   },
   "source": [
    "### This is Dev script, building several simple modelsto predict passenger survival from Kaggle Titanic dataset\n",
    "\n",
    "Copied from my Kaggle notebook, v.17\n",
    "\n",
    "Since the adatset is very small, xgboost does not dominate older classic models.\n",
    "Those models require more feature engineering, so this script i somewhat more complex than the usual "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67467ab0-e68e-4fd3-a258-cf39313ea203",
   "metadata": {},
   "source": [
    "### Outline:\n",
    "0. Load libraries and custom functions.\n",
    "1. Load data.\n",
    "2. Preliminary data analysis: explore features and a target, delete unneeded features, create new features.\n",
    "3. Train-test split.\n",
    "4. Missing values. In some cases it may be useful to explore skew and perform log-transform before imputing missing values.\n",
    "5. Feature engineering. Transform skewed variables, do OHC and scaling.\n",
    "6. Fit models.\n",
    "7. Evaluate models.\n",
    "8. Feature importance, error analysis. Based on the results, go to 2. and iterate.\n",
    "9. Make predictions.\n",
    "\n",
    "### To do:\n",
    "- Add EDA visualization.\n",
    "- Add SHAP feature importances.\n",
    "- Add Optuna XGBoost hyperparameter tuning.\n",
    "- Add PR curve analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18b89055-6964-448c-9226-118861858def",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. Load libraries #\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, time, warnings, shap, optuna\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import LabelBinarizer, LabelEncoder, OrdinalEncoder, OneHotEncoder, StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict, GridSearchCV, train_test_split, KFold\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier, VotingClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, precision_recall_curve, auc\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, roc_auc_score\n",
    "from sklearn.inspection import permutation_importance\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "pd.set_option('display.max_columns', 20)\n",
    "pd.set_option('mode.chained_assignment', None)\n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load custom pre-processing functions:\n",
    "\n",
    "def draw_histograms(df, variables, n_rows, n_cols):\n",
    "    # stolen from https://stackoverflow.com/questions/29530355/plotting-multiple-histograms-in-grid\n",
    "    fig=plt.figure()\n",
    "    for i, var_name in enumerate(variables):\n",
    "        ax=fig.add_subplot(n_rows,n_cols,i+1)\n",
    "        df[var_name].hist(bins=10,ax=ax)\n",
    "        ax.set_title(var_name+\" Distribution\")\n",
    "    fig.tight_layout()  \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def fillna_mp_i1(df_train, df_test, df_pred, num_features, cat_features, num_fill='median', cat_fill='mode'):\n",
    "    \"\"\"This function speeds up filling missing values for 3 main datasets using different imputation methods.\n",
    "    Later may replace it with some subclass.\n",
    "    Example: fillna_mp_i1(X_train, X_test, X_pred, num_cols, cat_cols)\"\"\"\n",
    "    # set df_pred to None if it does not exist\n",
    "    if not ((cat_fill=='mode') and (num_fill=='median')):\n",
    "        print ('Imputation method not Implemented yet!')\n",
    "        return None\n",
    "    \n",
    "    df_train[num_features] = df_train[num_features].fillna(value=df_train[num_features].median())\n",
    "    df_test[num_features] = df_test[num_features].fillna(value=df_train[num_features].median())\n",
    "    df_train[cat_features] = df_train[cat_features].fillna(value=df_train[cat_features].mode().iloc[0])\n",
    "    df_test[cat_features] = df_test[cat_features].fillna(value=df_train[cat_features].mode().iloc[0])\n",
    "    if (df_pred is not None):\n",
    "        df_pred[num_features] = df_pred[num_features].fillna(value=df_train[num_features].median())\n",
    "        df_pred[cat_features] = df_pred[cat_features].fillna(value=df_train[cat_features].mode().iloc[0])\n",
    "    df_train[num_features+cat_features].count\n",
    "    \n",
    "    all_good = (\n",
    "    (np.prod(df_train[num_features+cat_features].shape)==df_train[num_features+cat_features].count().sum()) and \n",
    "    (np.prod(df_test[num_features+cat_features].shape) == df_test[num_features+cat_features].count().sum()) and \n",
    "    (np.prod(df_pred[num_features+cat_features].shape) == df_pred[num_features+cat_features].count().sum()))\n",
    "    if (all_good):\n",
    "        print('Missing values imputed successfully')\n",
    "    else:\n",
    "        print('There are still some missing values...')\n",
    "    \n",
    "def add_misDummy_mp_i1(df_train, df_test, df_pred, features):\n",
    "    \"\"\"This function creates new dummy columns for missing features.\n",
    "    Example: add_misDummy_mp_i1(X_train, X_test, X_pred, ['Age'])\"\"\"\n",
    "    # set df_pred to None if it does not exist\n",
    "    for feature_name in features:\n",
    "        misColName = 'mis'+feature_name\n",
    "        df_train.loc[df_train[feature_name].isnull(), misColName]=1\n",
    "        df_train.loc[df_train[feature_name].notnull(), misColName]=0\n",
    "        df_test.loc[df_test[feature_name].isnull(), misColName]=1\n",
    "        df_test.loc[df_test[feature_name].notnull(), misColName]=0\n",
    "        if (df_pred is not None):\n",
    "            df_pred.loc[df_pred[feature_name].isnull(), misColName]=1\n",
    "            df_pred.loc[df_pred[feature_name].notnull(), misColName]=0\n",
    "   \n",
    "\n",
    "def discretize_mp_i1(df_train, df_test, df_pred, feature, ntiles, delete_feature=False):\n",
    "    \"\"\"This function divides a continuous feature into quantile groups.\n",
    "    Example: discretize_mp_i1(X_train, X_test, X_pred, 'Age', 15)\"\"\"\n",
    "    # set df_pred to None if it does not exist\n",
    "    _,bin = pd.qcut(df_train[feature], ntiles, retbins = True, labels = False, duplicates = 'drop')\n",
    "    df_train[feature+'Ntile'] = pd.cut(df_train[feature], labels=False, duplicates = 'drop', bins = bin ,include_lowest = True)\n",
    "    df_test[feature+'Ntile'] = pd.cut(df_test[feature], labels=False, duplicates = 'drop', bins = bin ,include_lowest = True)\n",
    "    if (df_pred is not None):\n",
    "        df_pred[feature+'Ntile'] = pd.cut(df_pred[feature], labels=False, duplicates = 'drop', bins = bin ,include_lowest = True)\n",
    "    if (delete_feature==True):\n",
    "        df_train.drop(columns=[feature], inplace=True)\n",
    "        df_test.drop(columns=[feature], inplace=True)\n",
    "        df_pred.drop(columns=[feature], inplace=True)\n",
    "    print('Discretized ',feature, ' into ', len(bin)-1, ' bins')\n",
    "\n",
    "\n",
    "def log_transformer_mp_i1(df_train, df_test, df_pred, feature_subset=False, min_skew=3):\n",
    "    \"\"\"This function divides a continuous feature into quantile groups.\n",
    "    Example: log_transformer_mp_i1(X_train, X_test, X_pred, feature_subset=num_cols)\"\"\"\n",
    "    # set df_pred to None if it does not exist\n",
    "    if (feature_subset==False):\n",
    "        features_totransform = df_train.columns\n",
    "    else:\n",
    "        features_totransform = feature_subset.copy()\n",
    "    skewed_vars = list(df_train.skew()[abs(df_train.skew())>min_skew].index)\n",
    "    for col in list(set(skewed_vars)&set(features_totransform)):\n",
    "        df_train[col] = np.log1p(df_train[col])\n",
    "        df_test[col] = np.log1p(df_test[col])\n",
    "        if (df_pred is not None):\n",
    "            df_pred[col] = np.log1p(df_pred[col])\n",
    "    print('Skewed columns log-transformed: ', list(set(skewed_vars)&set(features_totransform)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877846dd-f18b-4574-a65f-45b104949afc",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d575f9a-70f8-4e28-82ad-fb89b876bfb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dfd4a308-7136-497a-9c2e-112e15062f61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(891, 8) (418, 7)\n",
      "categorical features:  ['Pclass', 'Sex', 'SibSp', 'Parch', 'Embarked'] numerical features:  ['Fare', 'Age', 'Age2']\n",
      "(801, 8) (90, 8) (801, 1) (418, 8)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 801 entries, 825 to 863\n",
      "Data columns (total 8 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   Pclass    801 non-null    int64  \n",
      " 1   Sex       801 non-null    object \n",
      " 2   Age       646 non-null    float64\n",
      " 3   SibSp     801 non-null    int64  \n",
      " 4   Parch     801 non-null    int64  \n",
      " 5   Fare      801 non-null    float64\n",
      " 6   Embarked  799 non-null    object \n",
      " 7   Age2      646 non-null    float64\n",
      "dtypes: float64(3), int64(3), object(2)\n",
      "memory usage: 56.3+ KB\n",
      "Missing values imputed successfully\n",
      "Discretized  Age  into  12  bins\n",
      "Discretized  SibSp  into  4  bins\n",
      "Discretized  Parch  into  4  bins\n",
      "Skewed columns log-transformed:  ['Fare']\n",
      "Logistic  {'C': 30} 0.815230961298377 0.7958368125969224\n",
      "SVM  {'C': 1} 0.8414481897627965 0.8207980436597877\n",
      "KNN  {'n_neighbors': 6} 0.8539325842696629 0.8250228637321563\n",
      "RF  {'max_depth': 4, 'max_features': 5, 'n_estimators': 100} 0.8476903870162297 0.8193566344586266 21.849761247634888\n",
      "XGB  {'colsample_bytree': 0.6, 'eta': 0.04, 'max_depth': 3, 'n_estimators': 200, 'subsample': 0.8} 0.8751560549313359 0.853721817964929 6.776045083999634\n",
      "XGB 0.8226950354609929 0.7656765676567657 0.8888888888888888\n",
      "Out of Sample:\n",
      "Logistic  0.8111111111111111 0.7971342383107088\n",
      "SVM  0.8111111111111111 0.7911010558069381\n",
      "KNN  0.7888888888888889 0.7594268476621416\n",
      "RF  0.8222222222222222 0.7978883861236802\n",
      "XGB  0.8222222222222222 0.8069381598793364\n",
      "Total time  33.55630898475647\n",
      "VotingClassifier train  0.846441947565543 0.8241679589645712\n",
      "VotingClassifier test  0.8111111111111111 0.7911010558069381\n",
      "Total time for script:  34.73680925369263\n"
     ]
    }
   ],
   "source": [
    " \n",
    "# 1. Load data #\n",
    "\n",
    "time0 = time.time()\n",
    "\n",
    "os.chdir('/home/jupyter/projects_data/titanic')\n",
    "df = pd.read_csv('train.csv') \n",
    "\n",
    "df.drop(columns=['Name', 'Ticket', 'Cabin', 'PassengerId'],inplace=True)\n",
    "pred = pd.read_csv('test.csv')\n",
    "pred0 = pred.copy()\n",
    "pred.drop(columns=['Name', 'Ticket', 'Cabin', 'PassengerId'],inplace=True)\n",
    "\n",
    "print(df.shape, pred.shape)\n",
    "#df.head()\n",
    "\n",
    "# 2. EDA, adding features #\n",
    "\n",
    "#df.Survived.value_counts()\n",
    "df['Age2'] = df['Age']**2\n",
    "pred['Age2'] = pred['Age']**2\n",
    "\n",
    "# 3. Train-test split #\n",
    "\n",
    "train_y = df[['Survived']]\n",
    "train_x = df.drop(columns = ['Survived'])\n",
    "X_pred = pred.copy()\n",
    "\n",
    "#bin_cols = [col for col in train_x.columns if train_x[col].nunique()==2]\n",
    "cat_cols = [col for col in train_x.columns if train_x[col].nunique() in range(2,10)]\n",
    "num_cols = list(set(train_x.columns)-set(cat_cols))\n",
    "\n",
    "print('categorical features: ', cat_cols, 'numerical features: ', num_cols)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_x, train_y, test_size = 0.1, random_state=101)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, X_pred.shape)\n",
    "\n",
    "X_train.info()\n",
    "\n",
    "# 4. Misisng values #\n",
    "\n",
    "add_misDummy_mp_i1(X_train, X_test, X_pred, ['Age'])\n",
    "\n",
    "fillna_mp_i1(X_train, X_test, X_pred, num_cols, cat_cols)\n",
    "#[X_train.count(), X_test.count(), X_pred.count()]\n",
    "\n",
    "# extra feature engineering (manual)\n",
    "\n",
    "discretize_mp_i1(X_train, X_test, X_pred, 'Age', 15)\n",
    "discretize_mp_i1(X_train, X_test, X_pred, 'SibSp', 30)\n",
    "discretize_mp_i1(X_train, X_test, X_pred, 'Parch', 60)\n",
    "\n",
    "cat_cols.extend(['misAge', 'AgeNtile', 'SibSpNtile', 'ParchNtile'])\n",
    "cat_cols = list(set(cat_cols)-set(['SibSp', 'Parch']))\n",
    "\n",
    "\n",
    "# 5.Feature engineering #\n",
    "\n",
    "log_transformer_mp_i1(X_train, X_test, X_pred, feature_subset=num_cols)\n",
    "\n",
    "# in general, if I plan using raw ols, I should drop one group. o/w, it is beteer to leabe all ohc groups.\n",
    "\n",
    "feature_transformer = ColumnTransformer([\n",
    "    (\"num\", StandardScaler(), num_cols),\n",
    "    (\"cat\", OneHotEncoder(sparse = False, handle_unknown=\"ignore\"), cat_cols),\n",
    "    ])\n",
    "\n",
    "X_train = pd.DataFrame(feature_transformer.fit_transform(X_train), columns=feature_transformer.get_feature_names_out())\n",
    "X_test = pd.DataFrame(feature_transformer.transform(X_test), columns=feature_transformer.get_feature_names_out())\n",
    "X_pred = pd.DataFrame(feature_transformer.transform(X_pred), columns=feature_transformer.get_feature_names_out())\n",
    "\n",
    "fewfeatures = ['num__Age', 'num__Age2', 'num__Fare', 'cat__Sex_male', 'cat__Pclass_2', 'cat__Pclass_3']\n",
    "\n",
    "X_train\n",
    "\n",
    "# 6. Fit models #\n",
    "\n",
    "lr = LogisticRegression()\n",
    "param_grid = {'C':[0.3, 1, 3, 10, 30]}\n",
    "lrm = GridSearchCV(lr, param_grid, cv=8)\n",
    "lrm.fit(X_train, y_train)\n",
    "print('Logistic ', lrm.best_params_, accuracy_score(y_train, lrm.predict(X_train)), roc_auc_score(y_train, lrm.predict(X_train)))\n",
    "\n",
    "svm = SVC()\n",
    "param_grid = {'C':[0.3, 1, 2, 3, 10]}\n",
    "svmm = GridSearchCV(svm, param_grid, cv=8)\n",
    "svmm.fit(X_train, y_train)\n",
    "print('SVM ', svmm.best_params_, accuracy_score(y_train, svmm.predict(X_train)), roc_auc_score(y_train, svmm.predict(X_train)))\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "param_grid = dict(n_neighbors=range(2,20))\n",
    "knnm = GridSearchCV(knn, param_grid, cv=8)\n",
    "knnm.fit(X_train[fewfeatures], y_train)\n",
    "print('KNN ', knnm.best_params_, accuracy_score(y_train, knnm.predict(X_train[fewfeatures])), roc_auc_score(y_train, knnm.predict(X_train[fewfeatures])))\n",
    "\n",
    "time1 = time.time()\n",
    "rf = RandomForestClassifier()\n",
    "param_grid = {'n_estimators':[100,200], 'max_depth':[2,4,6,8], 'max_features':[4,5,6]}\n",
    "rfm = GridSearchCV(rf, param_grid, cv=4)\n",
    "rfm.fit(X_train, y_train)\n",
    "print('RF ', rfm.best_params_, accuracy_score(y_train, rfm.predict(X_train)), roc_auc_score(y_train, rfm.predict(X_train)), time.time()-time1)\n",
    "\n",
    "time1 = time.time()\n",
    "xgb = XGBClassifier()\n",
    "# use 'gpu_hist' for more than 10,000 examples.\n",
    "param_grid = {'n_estimators':[200], \n",
    "              'max_depth':[3,4], \n",
    "              'eta':[0.03, 0.04, 0.05], \n",
    "              'subsample':[0.8],\n",
    "             'colsample_bytree':[ 0.6]}\n",
    "xgbm = GridSearchCV(xgb, param_grid, cv=2)\n",
    "xgbm.fit(X_train, y_train)\n",
    "print('XGB ', xgbm.best_params_, accuracy_score(y_train, xgbm.predict(X_train)), roc_auc_score(y_train, xgbm.predict(X_train)), time.time()-time1)\n",
    "print('XGB', f1_score(y_train,xgbm.predict(X_train)), recall_score(y_train,xgbm.predict(X_train)), precision_score(y_train,xgbm.predict(X_train)))\n",
    "\n",
    "# 7. accuracy #\n",
    "\n",
    "print('Out of Sample:')\n",
    "print('Logistic ', accuracy_score(y_test, lrm.predict(X_test)), roc_auc_score(y_test, lrm.predict(X_test)))\n",
    "print('SVM ', accuracy_score(y_test, svmm.predict(X_test)), roc_auc_score(y_test, svmm.predict(X_test)))\n",
    "print('KNN ', accuracy_score(y_test, knnm.predict(X_test[fewfeatures])), roc_auc_score(y_test, knnm.predict(X_test[fewfeatures])))\n",
    "print('RF ', accuracy_score(y_test, rfm.predict(X_test)), roc_auc_score(y_test, rfm.predict(X_test)))\n",
    "print('XGB ', accuracy_score(y_test, xgbm.predict(X_test)), roc_auc_score(y_test, xgbm.predict(X_test)))\n",
    "print('Total time ', time.time()-time0)\n",
    "\n",
    "# VotingClassifier:\n",
    "\n",
    "estimator = []\n",
    "#estimator.append(('LR', LogisticRegression(C=1)))\n",
    "estimator.append(('SVM', SVC(C=1, probability = True)))\n",
    "#estimator.append(('KNN', KNeighborsClassifier(n_neighbors=5)))\n",
    "estimator.append(('RF', RandomForestClassifier(max_depth=5, max_features=4, n_estimators=200)))\n",
    "estimator.append(('XGB', XGBClassifier(eta=0.04, max_depth=3, n_estimators=200, \n",
    "                                       subsample=0.6, colsample_bytree=0.6)))\n",
    "vot_soft = VotingClassifier(estimators = estimator, voting ='soft')\n",
    "vot_soft.fit(X_train, y_train)\n",
    "print('VotingClassifier train ', accuracy_score(y_train, vot_soft.predict(X_train)), roc_auc_score(y_train, vot_soft.predict(X_train)))\n",
    "print('VotingClassifier test ', accuracy_score(y_test, vot_soft.predict(X_test)), roc_auc_score(y_test, vot_soft.predict(X_test)))\n",
    "\n",
    "print('Total time for script: ', time.time() - time0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8feb97b-3e02-46ab-bb3d-d5efab081726",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m98",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m98"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
