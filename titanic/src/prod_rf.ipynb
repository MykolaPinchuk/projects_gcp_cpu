{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52850930-9539-4beb-81cc-66476b59a4f6",
   "metadata": {},
   "source": [
    "### This is Prod script, building and deploying simple XGB model for Titanic\n",
    "\n",
    "Since it uses only RF and XGBoost, it is simpler than dev script for this project\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16bfb71f-bb6b-411f-a128-5c1e61b6487f",
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_run_id = 2\n",
    "# notebook_run_id is a digit, creating and deploying a new model every time this notebook is run. increment it by 1.\n",
    "project_name = 'My First Project'\n",
    "project_id = 'quantum-keep-360100'\n",
    "regionn = 'us-central1'\n",
    "\n",
    "ml_project_name = 'titanic'\n",
    "model_name = 'RF'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "32a220d1-04c2-42fb-8bca-d564de61cc48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. Load libraries #\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import os, time, warnings, optuna, pickle\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
    "from sklearn.preprocessing import LabelBinarizer, LabelEncoder, OrdinalEncoder, OneHotEncoder, StandardScaler\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict, GridSearchCV, train_test_split, KFold\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, precision_recall_curve, auc\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, roc_auc_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from google.cloud import bigquery, storage\n",
    "\n",
    "pd.set_option('display.max_columns', 20)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load custom pre-processing functions:\n",
    "\n",
    "def fillna_mp_i1(df_train, df_test, df_pred, num_features, cat_features, num_fill='median', cat_fill='mode'):\n",
    "    \"\"\"This function speeds up filling missing values for 3 main datasets using different imputation methods.\n",
    "    Later may replace it with some subclass.\n",
    "    Example: fillna_mp_i1(X_train, X_test, X_pred, num_cols, cat_cols)\"\"\"\n",
    "    # set df_pred to None if it does not exist\n",
    "    if not ((cat_fill=='mode') and (num_fill=='median')):\n",
    "        print ('Imputation method not Implemented yet!')\n",
    "        return None\n",
    "    \n",
    "    df_train[num_features] = df_train[num_features].fillna(value=df_train[num_features].median())\n",
    "    df_test[num_features] = df_test[num_features].fillna(value=df_train[num_features].median())\n",
    "    df_train[cat_features] = df_train[cat_features].fillna(value=df_train[cat_features].mode().iloc[0])\n",
    "    df_test[cat_features] = df_test[cat_features].fillna(value=df_train[cat_features].mode().iloc[0])\n",
    "    if (df_pred is not None):\n",
    "        df_pred[num_features] = df_pred[num_features].fillna(value=df_train[num_features].median())\n",
    "        df_pred[cat_features] = df_pred[cat_features].fillna(value=df_train[cat_features].mode().iloc[0])\n",
    "    df_train[num_features+cat_features].count\n",
    "    \n",
    "    all_good = (\n",
    "    (np.prod(df_train[num_features+cat_features].shape)==df_train[num_features+cat_features].count().sum()) and \n",
    "    (np.prod(df_test[num_features+cat_features].shape) == df_test[num_features+cat_features].count().sum()) and \n",
    "    (np.prod(df_pred[num_features+cat_features].shape) == df_pred[num_features+cat_features].count().sum()))\n",
    "    if (all_good):\n",
    "        print('Missing values imputed successfully')\n",
    "    else:\n",
    "        print('There are still some missing values...')\n",
    "    \n",
    "def add_misDummy_mp_i1(df_train, df_test, df_pred, features):\n",
    "    \"\"\"This function creates new dummy columns for missing features.\n",
    "    Example: add_misDummy_mp_i1(X_train, X_test, X_pred, ['Age'])\"\"\"\n",
    "    # set df_pred to None if it does not exist\n",
    "    for feature_name in features:\n",
    "        misColName = 'mis'+feature_name\n",
    "        df_train.loc[df_train[feature_name].isnull(), misColName]=1\n",
    "        df_train.loc[df_train[feature_name].notnull(), misColName]=0\n",
    "        df_test.loc[df_test[feature_name].isnull(), misColName]=1\n",
    "        df_test.loc[df_test[feature_name].notnull(), misColName]=0\n",
    "        if (df_pred is not None):\n",
    "            df_pred.loc[df_pred[feature_name].isnull(), misColName]=1\n",
    "            df_pred.loc[df_pred[feature_name].notnull(), misColName]=0\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e582e58-ad0c-4982-8c0a-84a4afc9579d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(891, 8) (418, 7)\n"
     ]
    }
   ],
   "source": [
    "# 1. Load data #\n",
    "\n",
    "time0 = time.time()\n",
    "\n",
    "os.chdir('/home/jupyter/projects_data/titanic')\n",
    "df = pd.read_csv('train.csv') \n",
    "\n",
    "df.drop(columns=['Name', 'Ticket', 'Cabin', 'PassengerId'],inplace=True)\n",
    "pred = pd.read_csv('test.csv')\n",
    "pred.drop(columns=['Name', 'Ticket', 'Cabin', 'PassengerId'],inplace=True)\n",
    "\n",
    "print(df.shape, pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "865174ee-c143-4bcd-9a33-848a88bd2b36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "categorical features:  ['Sex', 'Embarked'] numerical features:  ['Parch', 'SibSp', 'Fare', 'Age', 'Pclass', 'Age2']\n",
      "(712, 8) (179, 8) (712, 1) (418, 8)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 712 entries, 42 to 122\n",
      "Data columns (total 8 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   Pclass    712 non-null    int64  \n",
      " 1   Sex       712 non-null    object \n",
      " 2   Age       570 non-null    float64\n",
      " 3   SibSp     712 non-null    int64  \n",
      " 4   Parch     712 non-null    int64  \n",
      " 5   Fare      712 non-null    float64\n",
      " 6   Embarked  710 non-null    object \n",
      " 7   Age2      570 non-null    float64\n",
      "dtypes: float64(3), int64(3), object(2)\n",
      "memory usage: 50.1+ KB\n"
     ]
    }
   ],
   "source": [
    "# 2. EDA, adding features #\n",
    "\n",
    "df['Age2'] = df['Age']**2\n",
    "pred['Age2'] = pred['Age']**2\n",
    "\n",
    "# 3. Train-test split #\n",
    "\n",
    "train_y = df[['Survived']]\n",
    "train_x = df.drop(columns = ['Survived'])\n",
    "X_pred = pred.copy()\n",
    "\n",
    "cat_cols = ['Sex', 'Embarked']\n",
    "num_cols = list(set(train_x.columns)-set(cat_cols))\n",
    "\n",
    "print('categorical features: ', cat_cols, 'numerical features: ', num_cols)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_x, train_y, test_size = 0.2, random_state=4)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, X_pred.shape)\n",
    "\n",
    "X_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ffd6796a-a6df-4c60-a9fb-7fedcd5b6cf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values imputed successfully\n"
     ]
    }
   ],
   "source": [
    "# 4. Misisng values #\n",
    "\n",
    "add_misDummy_mp_i1(X_train, X_test, X_pred, ['Age'])\n",
    "\n",
    "fillna_mp_i1(X_train, X_test, X_pred, num_cols, cat_cols)\n",
    "\n",
    "cat_cols.extend(['misAge'])\n",
    "\n",
    "feature_transformer = ColumnTransformer([\n",
    "        (\"cat\", OneHotEncoder(sparse = False, handle_unknown=\"ignore\", drop='if_binary'), cat_cols)],\n",
    "        remainder = \"passthrough\"\n",
    "    )\n",
    "\n",
    "X_train = pd.DataFrame(feature_transformer.fit_transform(X_train), columns=feature_transformer.get_feature_names_out())\n",
    "X_test = pd.DataFrame(feature_transformer.transform(X_test), columns=feature_transformer.get_feature_names_out())\n",
    "X_pred = pd.DataFrame(feature_transformer.transform(X_pred), columns=feature_transformer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14974503-08fd-460f-9ec8-152f5b390150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF  {'max_depth': 4, 'max_features': 4, 'n_estimators': 200} \n",
      " 0.8370786516853933 0.8101929737753587 13.264614582061768\n",
      "XGB  {'colsample_bytree': 0.6, 'eta': 0.01, 'max_depth': 4, 'n_estimators': 250, 'subsample': 0.7} \n",
      " 0.8581460674157303 0.8386194952993569 33.47432899475098\n",
      "Out of Sample:\n",
      "RF  0.8547486033519553 0.8039915966386554\n",
      "XGB  0.8212290502793296 0.7829131652661064\n",
      "Total time  46.89345169067383\n",
      "Total time for training part:  46.89351725578308\n"
     ]
    }
   ],
   "source": [
    "# 6. Fit models #\n",
    "\n",
    "time1 = time.time()\n",
    "rf = RandomForestClassifier()\n",
    "param_grid = {'n_estimators':[100, 200], \n",
    "              'max_depth':[3, 4, 5, 6, 7], \n",
    "              'max_features':[4, 5, 6]}\n",
    "rfm = GridSearchCV(rf, param_grid, cv=2)\n",
    "rfm.fit(X_train, y_train)\n",
    "print('RF ', \n",
    "      rfm.best_params_, \n",
    "      '\\n',\n",
    "      accuracy_score(y_train, rfm.predict(X_train)), \n",
    "      roc_auc_score(y_train, rfm.predict(X_train)), time.time()-time1)\n",
    "\n",
    "time1 = time.time()\n",
    "xgb = XGBClassifier()\n",
    "# use 'gpu_hist' for more than 10,000 examples.\n",
    "param_grid = {'n_estimators':[150, 250], \n",
    "              'max_depth':[2, 3, 4], \n",
    "              'eta':[0.01, 0.02, 0.03, 0.04, 0.05, 0.06], \n",
    "              'subsample':[0.7],\n",
    "              'colsample_bytree':[0.6]}\n",
    "xgbm = GridSearchCV(xgb, param_grid, cv=2)\n",
    "xgbm.fit(X_train, y_train)\n",
    "print('XGB ', \n",
    "      xgbm.best_params_, \n",
    "      '\\n',\n",
    "      accuracy_score(y_train, xgbm.predict(X_train)), \n",
    "      roc_auc_score(y_train, xgbm.predict(X_train)), \n",
    "      time.time()-time1)\n",
    "\n",
    "\n",
    "# 7. model evaluation #\n",
    "\n",
    "print('Out of Sample:')\n",
    "print('RF ', \n",
    "      accuracy_score(y_test, rfm.predict(X_test)), \n",
    "      roc_auc_score(y_test, rfm.predict(X_test)))\n",
    "print('XGB ', \n",
    "      accuracy_score(y_test, xgbm.predict(X_test)), \n",
    "      roc_auc_score(y_test, xgbm.predict(X_test)))\n",
    "print('Total time ', time.time()-time0)\n",
    "\n",
    "print('Total time for training part: ', time.time() - time0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd93e71-29bb-4d1a-8c21-2f57d89093a5",
   "metadata": {},
   "source": [
    "The results are somewhat surprising. I have played for more than 1 hours with hyprparmeters and RF still usually beats XGB. \n",
    "If I do hyperparemter tuning rigorously (e.g., Optuna), xgb will probably beat RF eventually. But do not want to waste more time on this, given that thi is Prod script. So I use RF."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb65b8e3-3eae-41fb-81c0-f1c9989fd181",
   "metadata": {},
   "source": [
    "#### RF Model deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e446569-1a1f-48b7-81da-d33861fc74e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment_time_start = time.time()\n",
    "\n",
    "os.chdir('/home/jupyter/projects_gcp_cpu')\n",
    "\n",
    "model_path = os.getcwd()+'/titanic/artifacts/model_rf/'\n",
    "\n",
    "# Save model artifact to local filesystem (doesn't persist)\n",
    "artifact_filename = 'model.pkl'\n",
    "with open(model_path+artifact_filename, 'wb') as model_file:\n",
    "  pickle.dump(rfm, model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a71550d-31e1-4c89-80e5-fddc2a7c78ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload model artifact to Cloud Storage\n",
    "# Change the model directory to your GCS bucket URI\n",
    "model_bucket = 'gs://pmykola-projectsgcp-artifacts/titanic-rf'\n",
    "storage_path = os.path.join(model_bucket, artifact_filename)\n",
    "blob = storage.blob.Blob.from_string(storage_path, client=storage.Client(project=project_id))\n",
    "# previously it was 'project_id'\n",
    "blob.upload_from_filename(model_path+artifact_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da757d60-acf5-47e2-be43-031b52cbbcab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Model\n",
      "Create Model backing LRO: projects/234443118908/locations/us-central1/models/6791775683749085184/operations/1273007209655042048\n",
      "Model created. Resource name: projects/234443118908/locations/us-central1/models/6791775683749085184@1\n",
      "To use this Model in another session:\n",
      "model = aiplatform.Model('projects/234443118908/locations/us-central1/models/6791775683749085184@1')\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "# Use this line so we do not need to explicitly specify the project number and region whenever we use AI Platform (Vertex AI) services\n",
    "aiplatform.init(project=project_id, location=regionn)\n",
    "\n",
    "# Importing model artifacts\n",
    "model = aiplatform.Model.upload(display_name = ml_project_name+model_name+str(notebook_run_id),\n",
    "    description = ml_project_name+model_name+str(notebook_run_id),\n",
    "    artifact_uri = model_bucket,\n",
    "    serving_container_image_uri = 'us-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.1-0:latest'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21d0307b-5ed9-4969-8b32-c6cc0de2d555",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Endpoint\n",
      "Create Endpoint backing LRO: projects/234443118908/locations/us-central1/endpoints/7532134037335310336/operations/1604584732220194816\n",
      "Endpoint created. Resource name: projects/234443118908/locations/us-central1/endpoints/7532134037335310336\n",
      "To use this Endpoint in another session:\n",
      "endpoint = aiplatform.Endpoint('projects/234443118908/locations/us-central1/endpoints/7532134037335310336')\n"
     ]
    }
   ],
   "source": [
    "endpoint = aiplatform.Endpoint.create(display_name = ml_project_name+model_name+str(notebook_run_id), \n",
    "                                      project = project_id, \n",
    "                                      location = regionn)\n",
    "endpoint_id = endpoint.resource_name[-19:0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "444a9410-15e8-47a7-b863-88813e5cc0a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deploying model to Endpoint : projects/234443118908/locations/us-central1/endpoints/7532134037335310336\n",
      "Deploy Endpoint model backing LRO: projects/234443118908/locations/us-central1/endpoints/7532134037335310336/operations/2671937843907002368\n",
      "Endpoint model deployed. Resource name: projects/234443118908/locations/us-central1/endpoints/7532134037335310336\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<google.cloud.aiplatform.models.Endpoint object at 0x7fc01a8d8d10> \n",
       "resource name: projects/234443118908/locations/us-central1/endpoints/7532134037335310336"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# even RF is slow to deploy...\n",
    "# 14 minutes. I have no idea why it is so slow...\n",
    "# have to experiment with more powerful machines, maybe they will work faster\n",
    "model.deploy(endpoint = endpoint,\n",
    "             machine_type = 'n1-standard-2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bdd15228-d3af-4508-8943-f7a0b45398d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prediction(predictions=[0.0], deployed_model_id='3037875860754399232', model_version_id='1', model_resource_name='projects/234443118908/locations/us-central1/models/6791775683749085184', explanations=None)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'7532134037335310336'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(endpoint.predict(instances=[[1.0, 1.0, 0.0, 0.0, 1.0, 3.0, 28.5, 0.0, 0.0, 7.8958, 812.25]]))\n",
    "endpoint_id = endpoint.resource_name[-19:]\n",
    "display(endpoint_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1781a48e-310e-41b9-95bf-6ab06c4091ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using endpoint [https://us-central1-prediction-aiplatform.googleapis.com/]\n",
      "[0, 1]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# first guy must die, the second should survive.\n",
    "payload = {'instances': [[1.0, 0.0, 0.0, 1.0, 1.0, 3.0, 28.5, 0.0, 0.0, 7.8958, 812.25], \n",
    "                         [0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 34.0, 0.0, 0.0, 400, 812.25]]}\n",
    "\n",
    "# Parse JSON\n",
    "with open('request.json', 'w') as outfile:\n",
    "    json.dump(payload, outfile)\n",
    "\n",
    "!gcloud ai endpoints predict $endpoint_id \\\n",
    "  --region=$regionn \\\n",
    "  --json-request=request.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7cfbb82f-27f0-48f8-8ebf-bc715952e656",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model deployment time:  833.2911560535431\n"
     ]
    }
   ],
   "source": [
    "print('Model deployment time: ', time.time() - deployment_time_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df3627a-6832-45f1-9d2f-4b7c8795cc29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900fd509-6621-4560-97e6-21d51fa9b1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete_model = True\n",
    "# delete_endpoint = True\n",
    "\n",
    "# if delete_endpoint:\n",
    "#     try:\n",
    "#         endpoint.undeploy_all()\n",
    "#         endpoint.delete()\n",
    "#     except Exception as e:\n",
    "#         print(e)\n",
    "\n",
    "# if delete_model:\n",
    "#     try:\n",
    "#         model.delete()\n",
    "#     except Exception as e:\n",
    "#         print(e)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m98",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m98"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
